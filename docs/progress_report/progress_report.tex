\def\year{2023}\relax
%File: progress_report.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai21}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%\nocopyright
%PDF Info Is REQUIRED.
% For /Author, add all authors within the parentheses, separated by commas. No accents or commands.
% For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses.

\pdfinfo{
/Title QWOP Bot Progress Report
/Author Matthew Oros, Sonny Smith, Michael Terekhov
/TemplateVersion (2021.2)
} %Leave this
% /Title ()
% Put your actual complete title (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case
% Leave the space between \Title and the beginning parenthesis alone
% /Author ()
% Put your actual complete list of authors (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case.
% Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands,
% remove them.

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai21.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash

\title{QWOP Bot Progress Report}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Written by Matthew Oros, Sonny Smith, and Michael Terekhov \\ 
}
\affiliations{
    %Afiliationsz
    Baldwin Wallace University\\
    %If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    %For example,

    % Sunil Issar, \textsuperscript{\rm 2}
    % J. Scott Penberthy, \textsuperscript{\rm 3}
    % George Ferguson,\textsuperscript{\rm 4}
    % Hans Guesgen, \textsuperscript{\rm 5}.
    % Note that the comma should be placed BEFORE the superscript for optimum readability

    275 Eastland Road\\
    Berea, Ohio 44017\\
    % email address must be in roman text type, not monospace or sans serif

    % See more examples next
}
\iffalse
%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Single Author}
\author {
    % Author
    Author Name \\
}

\affiliations{
    Affiliation \\
    Affiliation Line 2 \\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name,\textsuperscript{\rm 1}
    Second Author Name, \textsuperscript{\rm 2}
    Third Author Name \textsuperscript{\rm 1} \\
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1} Affiliation 1 \\
    \textsuperscript{\rm 2} Affiliation 2 \\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi
\begin{document}

\maketitle

\begin{abstract}
QWOP is a challenging 2-dimensional game where the goal is to make the main character walk/run to the right as far as possible without falling.
The game's difficulty lies in its physics simulation, which makes movements hard to predict and requires a quick reaction time as well as an understanding of how the combinations of key presses translate to a desired movement on the screen.
The character is controlled using four keyboard keys: Q, W, O, and P. Q and W control the thighs, and O and P control the knees by either rotating the joints one way or the other.
As soon as an upper-body limb contacts the flat ground, the game is over and is restarted.
The score of the game is based on the distance traveled by the character.\\
In this project, we propose a simulation that allows an agent to learn to control the character and play the game of QWOP. Two main implementations are considered and attempted, both based on neural networks.
The first method is a genetic algorithm approach to learning, while the second approach is a deep Q-learning method.\\
The genetic approach is inspired by natural selection where the best performers get to reproduce and spread their genes to offspring.
The new population then replaces the old one and plays the game again, and the process is repeated until the agents play the game well enough.\\
The deep Q-learning approach is a powerful reinforcement learning method that uses neural networks to approximate the Q-function, which measures the expected reward of taking a certain action in a given state.
By combining the benefits of deep learning and reinforcement learning, the deep Q-learning approach can learn to make optimal decisions in a wide range of environments, even in situations where the optimal policy is not known in advance.\\
\end{abstract}

\noindent Content Here

\section{Problem Description}
[TODO: Do we want to keep the description of the game in here or in abstract?]
QWOP is a simple 2-dimensional game with the goal being to make the main character run to the right as far as possible without falling.
The challenge of the game is its physics simulation.
The character is controlled using four keyboard keys: Q, W, O, and P. Q and W control the thighs [TODO: Check if this is the right way around] and O and P control the knees by either rotating the joints one way or the other.
As soon as an upper-body limb contacts the flat ground, the game is over and is restarted.
The score of the game is based on the distance traveled by the character. \\
\indent The game is intentionally difficult as the physics simulation makes movements hard to predict and require a quick reaction time as well as understanding of how the combinations of key pressed translate to a desired movement on screen.

\section{Proposed Implementation}
The proposal of the simulation is an agent that is able to learn on its own to control the character to play the game of QWOP.
Two main implementations that will be considered and attempted will be both based on neural networks [TODO: Make sure we are still using neural networks for Deep Q].
The first method is a genetic algorithm approach to learning while the second approach will be a deep Q method of learning.


\section{Consideration of Backpropagation}
A traditional method training a neural network is to use a method called backpropagation.
Backpropagation is done by providing the network with input data and labelled output data where the term labelled refers to the desired output of the given input.
The input is fed forward through the network in its current state and its output is compared to the labelled output.
The weights are then adjusted from back to front based on how incorrect the network's output is. \\
\indent The difficulty with this approach for this domain of problem is that there is no simpel method of obtaining such labeled data in this problem.
The game of QWOP is difficult for humans and such the goal of the simulation is for the learning process to generate a solution from its fitness feedback and environment rather than by example from human playthrough.
Thus, the two methods that were chosen incorporate some form of random exploration of the state space to converge on a desired solution.\\
\section{Genetic approach}
The genetic approach is inspired by natural selection.
In each generation, a specific amount of agents is created and each has a distinct neural network(genes).
These genes can be mutated, recombined, and evaluated for fitness.
Based on their fitness, individuals are selected for reproduction, which involves combining their genes to create new children.
The new population then replaces the old one, and the process is repeated until the agents play the game well enough.\\

\section{Deep-Q-Learning Approach}
The proposed method is based on deep Q-learning, a model-free, bootstrapped, off-policy learning algorithm.
This implies that the agent does not require any prior knowledge of the environment dynamics.
The agent learns by interacting with the environment and collecting experience samples.
The algorithm constructs estimates of the action-value functions, which represent the expected return of taking each action in a given state, based on previous estimates.
This is a bootstrapping process, where one estimate is used to update another.
The algorithm also employs an off-policy strategy, where it uses an epsilon-greedy policy to generate actions that explore the state-action space with a probability epsilon and exploit the current best action with a probability 1-epsilon.
The data generated by this exploratory policy is used to update a purely greedy policy that maximizes the action-value function.\\

The agent possesses various types of memories, namely state memory, new state memory, action memory, reward memory, and terminal memory.
These memories are utilized to calculate action-value functions, which are crucial for learning.
The terminal memory stores the done flags that indicate when the exploration period ends and the states that need to be updated with the agent's estimates of the action-value Q are passed.
To enable the intelligent learning of the agent, batches of memories are employed.
The batches allow the data to be used for updating the agent's neural network when they are full.
By processing the training data in batches, the neural network can update its weights more frequently and efficiently.\\


Mean Squared Error (MSE) is a loss function that was applied in this project.
A loss function measures the difference between the estimated action-value function ($Q(s, a)$) and the target action-value function.
The target action-value function represents the expected return after taking an action in a given state, and it is computed as the sum of the immediate reward and gamma (discount factor) times the maximum value for the next state.
\\
\\*
$target = reward + gamma * maxQ(next state, a')$\\
\\
The function produces gradients with respect to the weights of the neural network.
The gradients are the measures of how much each weight contributes to the overall solution using backpropagation.\\

An optimizer, Adaptive Moment Estimation (Adam), is utilized.
Adam uses the gradients of the loss function to update the weights of the neural network that correspond to the global solution.
Adam knows in which direction the weights should be nudged by calculating the first and second moments of the gradient from the loss function, which are estimates of the mean and variance of the gradient.
Adam then adjusts the learning rate for each weight based on these moments, making it larger for weights with low variance and smaller for weights with high variance.
This way, Adam can find a good balance between exploring and exploiting the search space and converge faster to a minimum of the loss function.

\section{Current Progress}
\indent In order to create a suitable learning environment for the agent, the game was reimplemented.
The original game is browser-based and would require interfacing code to grab relavent data and to simulate keypresses.
The logistics of this are not relevent to the desired outcome of the project and thus the approach of reimplementation of the game was chosen. \\
\indent The programming language that was chosen was Python.
Python was chosen due to its simplicity, flexibility, and plentiful collection of libraries, especially related to machine learning.
For the simulation itself the graphics library chosen is Raylib.
Raylib is a native C library but has various bindings to other languages such as Python.
It was chosen due to its simplicity where it is not a graphics engine but rather abstracts OpenGl drawing calls to functions.
This helps to decouple the simulation from the graphics.
The physics library chosen is PyMunk.
It is a simple 2D physics library where physics bodies are created with certain shapes and properties and added to a simulation space.
PyMunk was also chosen for its simplicity and flexibility. \\
\indent The initial progress on the project started with a basic simulation of physics bodies where the joints that connect them were actuated by a key press.
This basic simulation later evolved to created legs and then hips, and then the upper body.
Various tweaks were needed to produce a correct feeling simulation based on the original game.
In order to accomplish this, the ability to directly control the character with the keyboard was added even though it would not be used in the final result as rather the simulated agent would be controlling the character. \\
\indent Once the progress on the physically-simulated character was acceptable, work begun on creating a neural network.
The neural network was [TODO: originally?] created without use of any libraries.
A neural network consists of input neurons that connect to hidden neurons which then connect to output neurons.
The strength of the connections between neurons are called weights.
Each neuron accepts a single or multiple floating point values which then get summed and normalized using an activation function.
The activation function chosen was the sigmoid function as it is very common choice. \\
\indent In order to utilize a neural network, the inputs that are fed to the network must be determined.
These inputs would denote which aspects of the environment the agent would be informed about.
If too little information is forwarded to the network, then it may not be able to converge on an acceptable solution.
If too much data is sent, the network may also have difficulty converging with having to weight the numerous inputs.
The current implementation sends the global x and y positions of each limb to the network.
We hypothesize that this would give the network enough information to determine how it should move its limbs when in a certain physical configuration.

\section{Future Considerations}
Some future considerations for the inputs of the neural network would be to send information regarding velocity of the character.
This might better inform the network.
However, it may not be necessary to input velocity for each individual limb and so an average velocity or velocity of just the center of mass might be sufficient.
However, this could only be determined through experimentation.
\indent Another idea might be to have a memory neuron which would be an output neuron whos output is connected to an input neuron.
This could potentially allow for the network to retain some type of memory about its computational state.
This may be a helpful addition as the problem being solved is highly temporal and so the ability for the network to retain some kind of information about the previous frame to the next could prove to be beneficial.


\end{document}
